# -*- coding: utf-8 -*-
"""GroupNo24._SportsPrediction

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11dJUDqcLoVXGOCtIfM7APwWbzc-CWp9-
"""

import pandas as pd
import os
import sklearn
import numpy as np
import matplotlib.pyplot as plt
from sklearn import tree, metrics
from sklearn.model_selection import train_test_split
from google.colab import drive
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import OneHotEncoder
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import RandomForestRegressor
from sklearn.linear_model import LinearRegression
from sklearn.feature_selection import RFE
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import random as rnd
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import cross_val_score
import xgboost as xgb
drive.mount('/content/drive')

pip install xgboost lightgbm

#Load the "players_21" dataset for training
df_21=pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_21 copy.csv')

#Load the "players_22" dataset for testing
df_22= pd.read_csv('/content/drive/My Drive/Colab Notebooks/players_22 copy.csv')

df_21.info()

df_22.info()

df_21.head()

df_21.tail()

plt.hist(df_21['overall'], bins=20)
plt.xlabel('overall')
plt.ylabel('Frequency')
plt.title('Distribution of Player Ratings (players_21)')
plt.show()

"""**Data Preparation and Feature Extraction**"""

#We need to check for missing values
missing_values = df_21.isnull().sum()
missing_values_22 = df_22.isnull().sum()

missing_values

#We need to handle missing values so We are filling the missing values with the mean
df_21.fillna(df_21.mean,inplace=True)
df_22.fillna(df_21.mean,inplace=True)

df_21 = df_21[df_21.columns.drop(list(df_21.filter(regex='url')))]
df_22 = df_22[df_22.columns.drop(list(df_22.filter(regex='url')))]

df_21

columns_to_drop= ['long_name','dob','sofifa_id','club_team_id']

df_21 = df_21.drop(columns=columns_to_drop)
df_22 = df_22.drop(columns=columns_to_drop)

df_21

"""Feature selection"""

#We need to perform a feature engineering.This is to create new features to improve the models performance
df_21['total_stats']=df_21[['weak_foot','pace','shooting','passing','dribbling','defending','physic']].sum(axis=1, numeric_only=True)

selected_columns = ['weak_foot', 'pace', 'shooting', 'passing', 'dribbling', 'defending', 'physic']

df_21.loc[:,'total_stats'] = df_21[selected_columns].sum(axis=1, numeric_only=True)
df_22.loc[:,'total_stats'] = df_22[selected_columns].sum(axis=1, numeric_only=True)

#Choose features that show the maximum correlation with the dependent variable. Over here, we chose the overall feature
correlations=df_21.corr(numeric_only=True)

correlation_overrall= correlations['overall'].abs().sort_values(ascending=False)

correlation_overrall

top=10

important_features=correlation_overrall[1:top+1].index

important_features

"""**Training with random regressor model**"""

# We need to Create a Random Forest Regressor model
ML_model= RandomForestRegressor(n_estimators=500,random_state=42)

from sklearn.preprocessing import StandardScaler
StandardScaler = StandardScaler()

#We split the data
X = df_21[important_features]
y = df_21['overall']
X

X= StandardScaler.fit_transform(X.copy())
X= pd.DataFrame(X, columns=important_features)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

ML_model = RandomForestRegressor(n_estimators=30,max_depth=30,min_samples_leaf=2,min_samples_split=5,random_state=42)
ML_model.fit(X_train, y_train)

# Make predictions on the validation set
y_pred = ML_model.predict(X_test)

y_pred
Mae = mean_absolute_error(y_test, y_pred)
Mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)
print(Mae)
print(Mse)
print(r2)

cross_val_score(ML_model, X_test, y_test, cv=5, scoring='neg_mean_squared_error')

feature_importances = ML_model.feature_importances_

feature_importances

feature_subset = df_21[important_features]

feature_subset.head()

"""**Model Selection and Training**

Using xgboost model
"""

import xgboost as xgb

ML_modelxgb= xgb.XGBRegressor()
ML_modelxgb.fit(X_train,y_train)

# cross validation
cross_val_score(ML_modelxgb, X_test, y_test, cv=5)

#measuring the performance
y_pred_xgb= ML_modelxgb.predict(X_test)
Mae= mean_absolute_error(y_test,y_pred_xgb)
Mse= mean_squared_error(y_test,y_pred_xgb)
r2= r2_score(y_test,y_pred_xgb)
print(Mae)
print(Mse)
print(r2)



"""**Model Performance**"""

#Define the hyperparameter grid to search
parameter_grid= {
    'n_estimators': [10, 20, 30],
    'max_depth': [10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
}

#We have to create a grid search instance
Grid_Search= GridSearchCV(ML_model,parameter_grid,cv=10,scoring='neg_mean_squared_error')

#Fitting the grid search into the training data
Grid_Search.fit(X_train,y_train)

models_predicted_fr= np.round(y_pred).astype(int)
model_predicted_xg=np.round(y_pred_xgb).astype(int)

#Confusion matrix
print((y_test,models_predicted_fr))
print((y_test,model_predicted_xg))

#We need to get the best hyperparameter
best_parameters=Grid_Search.best_params_

best_parameters

"""**Testing on fifa 22 players**"""

df_22.head()

df_22.loc[:,'total_stats'] = df_22[selected_columns].sum(axis=1)

correlations=df_22.corr()

correlation_overrall= correlations['overall'].abs().sort_values(ascending=False)
correlation_overrall

top=10

important_features=correlation_overrall[1:top+1].index
important_features

X=df_22[important_features]
y=df_22['overall']

StandardScaler=StandardScaler

X=StandardScaler.fit_transform(X.copy())
X=pd.DataFrame(X,columns=important_features)

"""Testing players performance with random regressor"""

ML_model=RandomForestRegressor(n_estimators=30,max_depth=30,min_samples_leaf=2)

ML_model.fit(X_train,y_train)

y_predict = ML_model.predict(X_test)

y_predict

print(mean_absolute_error(y_test, y_pred))

"""**Using xgboost model for predictions**"""

xgb_model = xgb.XGBRegressor()
xgb_model.fit(X_train, y_train)

y_predicted = xgb_model.predict(X_test)

print(mean_absolute_error(y_test, y_pred))

"""**Saving the model using pickle**"""

import pickle
import bz2

#saving as random_forest_regression_model
pkl_file = 'model.pkl'
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
with open("scaler.pkl","wb") as file:
  pickle.dump(scaler, file)
#oFile=bz2.BZ2File(pkl_file,'wb')
#oFile.close()

with open("ML_model.pkl","wb") as model_file:
  pickle.dump(ML_model, model_file)